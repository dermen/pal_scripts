#!/usr/bin/python3
import datetime
import os
import pylab as plt
import h5py
import numpy as np
import sys
from find_peaks5 import make_cxi_file2
import glob
from argparse import ArgumentParser
sys.path.append('/xfel/ffhs/dat/ue_180124/.asu_tools/lib/python')
from joblib import Parallel, delayed
import logging

parser = ArgumentParser(
    description='')

parser.add_argument(
    '-r1',
    dest='run1',
    type=int,
    default=None, help="run number1")

parser.add_argument(
    '-r2',
    dest='run2',
    type=int,
    default=None, help="run number2")

parser.add_argument(
    '-j',
    dest='num_jobs',
    type=int,
    default=None, help="number of jobs to use")

parser.add_argument(
    '-o, --output-dir',
    dest='outdir',
    type=str,
    default=None, help="where to store output files, defaults to results")

parser.add_argument(
    '--log-file',
    dest='log_file',
    type=str,
    default=None, help="default path to a log file...")

parser.add_argument(
    '-t',
    dest='tag',
    type=str,
    default='noname',help="a tag appended to output filename, \
        used to describe the run, e.g. ps2" )

parser.add_argument(
    '-X', '--fast-scan',
    dest='X',
    type=int,
    default=1440, help='image fast-scan dimension')

parser.add_argument(
    '-Y', '--slow-scan',
    dest='Y',
    type=int,
    default=1440, help='image slow-scan dimension')

parser.add_argument(
    '--gauss-variance',
    dest='sig_G',
    type=float,
    default=1.1, help='Blurring variance (in pixel units) used to blur and iimage prior to peak detection')

parser.add_argument(
    '--num-sigmas',
    dest='nsigs',
    type=float,
    default=4, help='Makes pixels 0 if there value is below this many standard deviations from the mean of the entire image')

parser.add_argument(
    '--min-dist',
    dest='min_dist',
    type=float,
    default=None, help='peaks too close will be analyzed, highest intrensity one will be selected')
parser.add_argument(
    '--thresh-val',
    dest='thresh',
    type=float,
    default=1, help='only pick a peak if its max intensity is above this value')
parser.add_argument(
    '--min-peaks',
    dest='min_num_pks',
    type=float,
    default=10, help='minimum number of peaks for an image to be considered a hit')

# filter extra
parser.add_argument(
    '--min-conn',
    dest='min_conn',
    type=int,
    default=3, help='minimum number of conneted pixels per peak') 
parser.add_argument(
    '--max-conn',
    dest='max_conn',
    type=int,
    default=18, help='maximum number of conneted pixels per peak') 
parser.add_argument(
    '--sz',
    dest='sz',
    type=int,
    default=4, help='size of the neighborhood when checking peaks')
parser.add_argument(
    '--min-snr',
    dest='min_snr',
    type=float,
    default=2., help='minimum SNR of a peak (uses medians)')
parser.add_argument(
    '--filter-peaks',
    dest='filt',
    action='store_true',
    help='whether to filter peaks by SNR and connectivity')


parser.add_argument(
    '--inside-radius',
    dest='r_in',
    default=None,
    type=float,
    help='peaks within this radius value will be detected, default ignored')
parser.add_argument(
    '--outside-radius',
    dest='r_out',
    default=None,
    type=float,
    help='peaks outside this radius value will be detected, default ignored')

parser.add_argument(
    '--use-salt-mask',
    dest='use_salt_mask',
    action='store_true',
    help='whether to additionally mask the salt ring.. (clen 300 only)') 

args = parser.parse_args()

if args.log_file is not None:
    log_file = args.log_file
else:
    log_time = str(datetime.datetime.now()).replace(' ','_')
    log_file = "/xfel/ffhs/dat/ue_180127/results/logs/%s.log"%log_time 
logging.basicConfig(filename=log_file, level=logging.DEBUG)
py_cmd = " ".join( sys.argv)
logging.info("Original command:\n%s"%py_cmd)

if args.outdir is None:
    outdir = "/xfel/ffhs/dat/ue_180127/results/multi_dump"
else:
    outdir = args.outdir
    if not os.path.exists(outdir):
        print("Dir %s does not exists for writing"%outdir)

if args.use_salt_mask:
    mask = np.load('/xfel/ffhs/dat/ue_180124/masks/mask_rings_clen300_LCP_21_WTpeptide.npy')
else:
    mask = np.load('/xfel/ffhs/dat/ue_180124/masks/maskallthree.npy')

#if args.mask_file is None:
#    mask = np.load('/xfel/ffhs/dat/ue_180124/masks/maskallthree.npy')
#else:
#    mask = np.load(args.mask_file)

def main(fname,outname, keys_, jid):
    fig = plt.figure(jid+1)
    ax = plt.Axes( fig, [0.,0.,1.,1.])
    ax.set_axis_off()
    fig.add_axes(ax)
    ax.imshow( np.zeros( (1440,1440)), cmap='viridis')
    plt.draw()
    plt.pause(0.001)

    outname = outname.replace(".cxi", "_job%d_%s.cxi"%(jid,args.tag)  )
    make_cxi_file2( fname, keys_, 
        mask=mask.astype(np.bool),
        outname = outname, 
        Hsh = (args.Y, args.X),	
        sig_G=args.sig_G,
        verbose=True, 
        thresh=args.thresh, 
        make_sparse=True,
        filt=args.filt,
        min_conn=args.min_conn,
        max_conn=args.max_conn,
        min_snr = args.min_snr,
        sz = args.sz,
        r_in=args.r_in,
        r_out=args.r_out,
        ninside=10,
        noutside=10,
        #sY=[520,920],
        #sX=[520,920],
        min_dist=args.min_dist, 
        nsigs=args.nsigs, 
        min_num_pks=args.min_num_pks,
        log_prefix="JOB %d:"%(jid+1),
        log_freq=5,
        ax=ax)

import time
r1 = args.run1
r2 = args.run2
for run in range( r1, r2+1):
    run_s = str(run).zfill(7)
   
    fname = [] 
    while not fname: 
        if run < 542:
            rundir = "/xfel/ffhs/dat/ue_180124/raw_data/%s/*.h5"%( run_s)
            fname = glob.glob(rundir)
        else:
            rundir = "/xfel/ffhs/dat/ue_180127/raw_data/%s/*.h5"%( run_s)
            fname = glob.glob(rundir)
        if fname:
            print("Found fnames", fname)
            try:
                test = h5py.File(fname[0], 'r')['header/frame_num'].value
            except OSError:
                print("File still writing.. sleeping for 10 seconds...")
                time.sleep(10)
                fname = []
    fname = fname[0] 
    outname = os.path.join( outdir, "run%d_HITS.cxi"%run)

    print("opening file %s"%fname)
    h5 = h5py.File( fname, 'r')
    keys = list( h5.keys() )[1:]
    keys_split = np.array_split(keys, args.num_jobs)
    h5.close()
    results = Parallel(n_jobs=args.num_jobs)(delayed(main)(fname, \
        outname, keys_split[jid], jid) \
        for jid in range( args.num_jobs)  )
    
    for jid, result in enumerate(results):
        if result:
            count,N, hit_rate = r
            logging.debug( "Run %d, Job %d : Found %d / %d hits (%.2f %%)"\
                %(run, jid, count, N,  hit_rate))
        else: 
            logging.debug("Run %d, Job %d : exited prematurely"%(run, jid))


